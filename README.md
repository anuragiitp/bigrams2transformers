
This repository outlines different **approaches to learn** language modeling concepts through a progression of increasingly sophisticated models:

1. **Count-Based Bigram Model** - Uses frequency statistics of character pairs
2. **Neural Bigram Model** - Simple neural network for next character prediction  
3. **MLP with Extended Context** - Multi-layer perceptron with richer context
4. **CNN-Inspired Hierarchical Model** - Processes sequences hierarchically
5. **Simple Transformer Model** - Self-attention based architecture

Each approach builds upon the previous one, making it ideal for educational purposes and understanding the evolution of modern NLP techniques.



- `four_models_comparison.ipynb` - Main notebook comparing different modeling approaches
- `simple_transformer_az.ipynb` - Focused implementation of a simple transformer for a-z character generation
