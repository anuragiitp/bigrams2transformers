<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simple Transformer Model Architecture</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 10px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.3);
            padding: 15px;
        }
        
        .title {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 5px;
            font-size: 2em;
            font-weight: bold;
        }
        
        .subtitle {
            text-align: center;
            color: #7f8c8d;
            margin-bottom: 15px;
            font-size: 1em;
        }
        
        .config-info {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 10px;
            margin-bottom: 15px;
            display: flex;
            justify-content: space-around;
            flex-wrap: wrap;
        }
        
        .config-item {
            text-align: center;
            margin: 3px;
        }
        
        .config-value {
            font-weight: bold;
            color: #3498db;
            font-size: 1.1em;
        }
        
        .config-label {
            color: #7f8c8d;
            font-size: 0.9em;
        }
        
        .diagram-container {
            display: flex;
            justify-content: center;
            margin: 15px 0;
        }
        
        .transformer-diagram {
            background: #ffffff;
            border-radius: 8px;
            padding: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .component {
            margin: 8px 0;
            padding: 10px;
            border-radius: 6px;
            text-align: center;
            position: relative;
            transition: all 0.3s ease;
        }
        
        .component:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.15);
        }
        
        .input-layer {
            background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
            color: #2c3e50;
        }
        
        .embedding-layer {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            color: #2c3e50;
        }
        
        .transformer-block {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            color: #2c3e50;
            border: 2px solid #e74c3c;
        }
        
        .attention-component {
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            color: #2c3e50;
            margin: 5px 15px;
            font-size: 0.85em;
        }
        
        .ffn-component {
            background: linear-gradient(135deg, #a8caba 0%, #5d4e75 100%);
            color: white;
            margin: 5px 15px;
            font-size: 0.85em;
        }
        
        .output-layer {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }
        
        .arrow {
            display: block;
            text-align: center;
            font-size: 1.5em;
            color: #3498db;
            margin: 5px 0;
        }
        
        .flow-text {
            font-size: 0.75em;
            color: #7f8c8d;
            margin: 3px 0;
        }
        
        .layer-norm {
            background: linear-gradient(135deg, #fad0c4 0%, #ffd1ff 100%);
            color: #2c3e50;
            margin: 3px 15px;
            padding: 6px;
            font-size: 0.75em;
            border-radius: 4px;
        }
        
        .residual-connection {
            border: 2px dashed #e74c3c;
            background: rgba(231, 76, 60, 0.1);
            margin: 3px 15px;
            padding: 6px;
            font-size: 0.75em;
            border-radius: 4px;
        }
        
        .math-formula {
            background: #2c3e50;
            color: white;
            padding: 6px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            margin: 5px 0;
            font-size: 0.8em;
        }
        
        .details-panel {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 15px;
            margin-top: 15px;
        }
        
        .details-title {
            color: #2c3e50;
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        
        .details-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 12px;
        }
        
        .detail-item {
            background: white;
            padding: 12px;
            border-radius: 6px;
            box-shadow: 0 1px 5px rgba(0,0,0,0.1);
        }
        
        .detail-item h4 {
            color: #3498db;
            margin-top: 0;
            margin-bottom: 8px;
            font-size: 1em;
        }
        
        .detail-item p {
            margin: 6px 0;
            font-size: 0.9em;
        }
        
        .detail-item ul {
            margin: 6px 0;
            padding-left: 18px;
        }
        
        .detail-item li {
            margin: 3px 0;
            font-size: 0.85em;
        }
        
        .export-button {
            position: fixed;
            top: 10px;
            right: 10px;
            background: #3498db;
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 20px;
            cursor: pointer;
            font-size: 0.9em;
            box-shadow: 0 2px 10px rgba(52, 152, 219, 0.3);
            transition: all 0.3s ease;
        }
        
        .export-button:hover {
            background: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(52, 152, 219, 0.4);
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            .config-info {
                flex-direction: column;
                padding: 8px;
            }
            .details-grid {
                grid-template-columns: 1fr;
                gap: 8px;
            }
            .detail-item {
                padding: 8px;
            }
            .export-button {
                top: 5px;
                right: 5px;
                padding: 6px 12px;
                font-size: 0.8em;
            }
        }
    </style>
</head>
<body>
    <button class="export-button" onclick="exportDiagram()">üì∏ Export as Image</button>
    
    <div class="container" id="diagram-container">
        <h1 class="title">Simple Transformer Model</h1>
        <p class="subtitle">Decoder-Only Autoregressive Architecture for Character-Level Text Generation</p>
        
        <div class="config-info">
            <div class="config-item">
                <div class="config-value">28</div>
                <div class="config-label">Vocab Size<br>(a-z + space + newline)</div>
            </div>
            <div class="config-item">
                <div class="config-value">64</div>
                <div class="config-label">Context Length<br>(Block Size)</div>
            </div>
            <div class="config-item">
                <div class="config-value">128</div>
                <div class="config-label">Embedding<br>Dimension</div>
            </div>
            <div class="config-item">
                <div class="config-value">8</div>
                <div class="config-label">Attention<br>Heads</div>
            </div>
            <div class="config-item">
                <div class="config-value">6</div>
                <div class="config-label">Transformer<br>Layers</div>
            </div>
            <div class="config-item">
                <div class="config-value">1.2M</div>
                <div class="config-label">Total<br>Parameters</div>
            </div>
        </div>
        
        <div class="diagram-container">
            <div class="transformer-diagram">
                <!-- Input Processing -->
                <div class="component input-layer">
                    <strong>üìù Input Text</strong><br>
                    <span class="flow-text">Example: "dogs are loyal and friendly animals that make great pets for families"</span><br>
                    <span class="flow-text">‚ö° Max Context: 64 characters at once</span>
                </div>
                
                <div class="arrow">‚Üì</div>
                
                <div class="component input-layer">
                    <strong>üî¢ Tokenization</strong><br>
                    <span class="flow-text">Characters ‚Üí Token IDs [3,14,6,18,26,0,17,4,26,11,14,24,0,11]</span>
                </div>
                
                <div class="arrow">‚Üì</div>
                
                <!-- Embedding Layer -->
                <div class="component embedding-layer">
                    <strong>üéØ Token Embedding</strong><br>
                    <span class="flow-text">Token IDs ‚Üí Dense Vectors (28 √ó 128)</span>
                    <div class="math-formula">token_emb = TokenEmbedding(token_ids)</div>
                </div>
                
                <div class="component embedding-layer">
                    <strong>üìç Positional Embedding</strong><br>
                    <span class="flow-text">Position indices ‚Üí Dense Vectors (64 √ó 128)</span>
                    <div class="math-formula">pos_emb = PositionalEmbedding(positions)</div>
                </div>
                
                <div class="component embedding-layer">
                    <strong>‚ûï Combined Embeddings</strong><br>
                    <span class="flow-text">x = token_emb + pos_emb</span>
                    <div class="math-formula">Shape: (batch_size, seq_len, 128)</div>
                </div>
                
                <div class="arrow">‚Üì</div>
                
                <!-- Transformer Blocks (6 layers) -->
                <div class="component transformer-block">
                    <strong>üîÑ Transformer Block √ó 6 Layers</strong>
                    
                    <!-- Layer Normalization 1 -->
                    <div class="layer-norm">
                        <strong>Layer Norm 1</strong><br>
                        <span class="flow-text">Normalize input for attention</span>
                    </div>
                    
                    <!-- Multi-Head Attention -->
                    <div class="attention-component">
                        <strong>üéØ Multi-Head Attention (8 heads)</strong><br>
                        <div class="math-formula">
                            Q, K, V = Linear(x)<br>
                            Attention = softmax(QK^T/‚àöd_k)V<br>
                            Head_size = 128/8 = 16<br>
                            Attention_Matrix = 64√ó64 (context_length¬≤)
                        </div>
                        <span class="flow-text">
                            ‚Ä¢ Query, Key, Value projections<br>
                            ‚Ä¢ Scaled dot-product attention<br>
                            ‚Ä¢ Causal masking (64√ó64 lower triangular)<br>
                            ‚Ä¢ Each position attends to ‚â§64 previous positions<br>
                            ‚Ä¢ Concatenate & project heads
                        </span>
                    </div>
                    
                    <!-- Residual Connection 1 -->
                    <div class="residual-connection">
                        <strong>‚ûï Residual Connection 1</strong><br>
                        <span class="flow-text">x = x + attention(norm(x))</span>
                    </div>
                    
                    <!-- Layer Normalization 2 -->
                    <div class="layer-norm">
                        <strong>Layer Norm 2</strong><br>
                        <span class="flow-text">Normalize input for feed-forward</span>
                    </div>
                    
                    <!-- Feed Forward Network -->
                    <div class="ffn-component">
                        <strong>üß† Feed Forward Network</strong><br>
                        <div class="math-formula">
                            FFN(x) = GELU(Linear1(x)) * Linear2<br>
                            Hidden_dim = 4 √ó 128 = 512
                        </div>
                        <span class="flow-text">
                            ‚Ä¢ Linear projection to 512 dims<br>
                            ‚Ä¢ GELU activation function<br>
                            ‚Ä¢ Linear projection back to 128 dims<br>
                            ‚Ä¢ Dropout for regularization
                        </span>
                    </div>
                    
                    <!-- Residual Connection 2 -->
                    <div class="residual-connection">
                        <strong>‚ûï Residual Connection 2</strong><br>
                        <span class="flow-text">x = x + ffn(norm(x))</span>
                    </div>
                </div>
                
                <div class="arrow">‚Üì</div>
                
                <!-- Final Processing -->
                <div class="component embedding-layer">
                    <strong>üìä Final Layer Norm</strong><br>
                    <span class="flow-text">Normalize final transformer output</span>
                </div>
                
                <div class="arrow">‚Üì</div>
                
                <div class="component output-layer">
                    <strong>üéØ Output Head</strong><br>
                    <span class="flow-text">Linear projection to vocabulary size</span>
                    <div class="math-formula">logits = Linear(x) ‚Üí (batch, seq_len, 28)</div>
                </div>
                
                <div class="arrow">‚Üì</div>
                
                <div class="component output-layer">
                    <strong>üìà Softmax & Sampling</strong><br>
                    <span class="flow-text">Convert logits to probabilities & sample next token</span>
                    <div class="math-formula">probs = softmax(logits / temperature)</div>
                </div>
                
                <div class="arrow">‚Üì</div>
                
                <div class="component input-layer">
                    <strong>üìù Generated Text Examples</strong><br>
                    <span class="flow-text">Decode token IDs back to characters</span>
                    <div class="math-formula">
                        üêï "dogs" ‚Üí "dogs come in many different breeds like gold"<br>
                        üê± "cats" ‚Üí "cats communicate through meowing purring and"<br>
                        üêò "elephants" ‚Üí "elephants live in family groups led by the oldest"<br>
                        ü¶á "bats" ‚Üí "bats hibernate during winter when insects ar"
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Detailed Information Panel -->
        <div class="details-panel">
            <div class="details-title">üîç Detailed Component Information</div>
            <div class="details-grid">
                <div class="detail-item">
                    <h4>üèóÔ∏è Decoder-Only Autoregressive Architecture</h4>
                    <p><strong>Decoder-Only:</strong> Uses only the decoder part of the original transformer, without an encoder. No cross-attention layers.</p>
                    <p><strong>Autoregressive:</strong> Generates text one token at a time, using previously generated tokens as context for predicting the next token.</p>
                    <p><strong>Key Features:</strong></p>
                    <ul>
                        <li>Causal (unidirectional) self-attention with masking</li>
                        <li>Cannot attend to future positions in the sequence</li>
                        <li>Perfect for text generation tasks</li>
                        <li>Similar to GPT, GPT-2, GPT-3 architecture</li>
                        <li>Trained with next-token prediction objective</li>
                    </ul>
                </div>
                <div class="detail-item">
                    <h4>üéØ Multi-Head Attention</h4>
                    <p><strong>Purpose:</strong> Allows the model to focus on different parts of the input sequence simultaneously.</p>
                    <p><strong>Key Features:</strong></p>
                    <ul>
                        <li>8 parallel attention heads</li>
                        <li>Each head has dimension 16 (128/8)</li>
                        <li>Causal masking prevents looking at future tokens</li>
                        <li>Scaled dot-product attention mechanism</li>
                    </ul>
                </div>
                
                <div class="detail-item">
                    <h4>üß† Feed Forward Network</h4>
                    <p><strong>Purpose:</strong> Provides non-linear transformations and adds model capacity.</p>
                    <p><strong>Architecture:</strong></p>
                    <ul>
                        <li>Input: 128 dimensions</li>
                        <li>Hidden: 512 dimensions (4√ó expansion)</li>
                        <li>Output: 128 dimensions</li>
                        <li>GELU activation function</li>
                    </ul>
                </div>
                
                <div class="detail-item">
                    <h4>üìç Context Length & Positional Embedding</h4>
                    <p><strong>Context Length:</strong> 64 characters maximum processing window.</p>
                    <p><strong>Positional Embedding Details:</strong></p>
                    <ul>
                        <li>Learned positional embeddings (0-63 positions)</li>
                        <li>Maximum sequence length: 64 characters</li>
                        <li>Each position gets unique embedding vector</li>
                        <li>Added to token embeddings element-wise</li>
                        <li>Enables position-aware attention patterns</li>
                    </ul>
                    <p><strong>Context Window Impact:</strong></p>
                    <ul>
                        <li>Memory usage: O(64¬≤) = 4,096 for attention matrix</li>
                        <li>Covers ~8-12 words of English text context</li>
                        <li>During generation: sliding window of last 64 chars</li>
                        <li>Training: sequences chunked into 64-character segments</li>
                    </ul>
                </div>
                
                <div class="detail-item">
                    <h4>üîÑ Residual Connections</h4>
                    <p><strong>Purpose:</strong> Enable training of deep networks by allowing gradients to flow directly.</p>
                    <p><strong>Implementation:</strong></p>
                    <ul>
                        <li>Skip connections around attention and FFN</li>
                        <li>Pre-normalization architecture</li>
                        <li>Helps with gradient flow</li>
                        <li>Enables stable training</li>
                    </ul>
                </div>
                
                <div class="detail-item">
                    <h4>üìä Training Process</h4>
                    <p><strong>Objective:</strong> Predict the next character in sequence.</p>
                    <p><strong>Details:</strong></p>
                    <ul>
                        <li>Cross-entropy loss function</li>
                        <li>AdamW optimizer (lr=3e-4)</li>
                        <li>10,000 training iterations</li>
                        <li>Batch size: 32</li>
                    </ul>
                </div>
                
                <div class="detail-item">
                    <h4>üéÆ Generation Process</h4>
                    <p><strong>Process:</strong> Autoregressive text generation with context window.</p>
                    <p><strong>Steps:</strong></p>
                    <ul>
                        <li>Start with input prompt (‚â§64 characters)</li>
                        <li>If prompt > 64 chars, use only last 64 characters</li>
                        <li>Pass context through transformer layers</li>
                        <li>Predict next token probabilities from last position</li>
                        <li>Sample from probability distribution (with temperature)</li>
                        <li>Append new token and slide context window</li>
                        <li>Repeat for desired number of tokens</li>
                    </ul>
                    <p><strong>Context Management:</strong></p>
                    <ul>
                        <li>Maintains sliding window of most recent 64 characters</li>
                        <li>Older context beyond 64 chars is "forgotten"</li>
                        <li>Enables indefinite length generation</li>
                    </ul>
                </div>
                
                <div class="detail-item">
                    <h4>üéØ Actual Model Performance</h4>
                    <p><strong>Training Results:</strong> Final loss: 0.1649 after 10,000 iterations</p>
                    <p><strong>Real Generation Examples:</strong></p>
                    <div style="background: #2c3e50; color: white; padding: 8px; border-radius: 4px; font-family: monospace; font-size: 0.8em; margin: 6px 0;">
                        <strong>Animal Prompts:</strong><br>
                        üêï "dogs" ‚Üí "dogs come in many different breeds like gold"<br>
                        üê± "cats" ‚Üí "cats communicate through meowing purring and"<br>
                        üê¶ "birds" ‚Üí "birds have excellent eyesight and can see col"<br>
                        üê† "fish" ‚Üí "fish and human garbage raccoons have highly"<br>
                        üêò "elephants" ‚Üí "elephants live in family groups led by the oldest"<br>
                        ü¶á "bats" ‚Üí "bats hibernate during winter when insects ar"<br><br>
                        
                        <strong>Temperature Effects on "dogs are":</strong><br>
                        üå°Ô∏è Temp 0.5: "dogs are small domesticated rodents th"<br>
                        üå°Ô∏è Temp 1.0: "dogs are nocturnal animals that sleep"<br>
                        üå°Ô∏è Temp 1.5: "dogs are omnivores that eat insects wo"
                    </div>
                    <p><strong>Model Observations:</strong></p>
                    <ul>
                        <li>Generates coherent animal-related text</li>
                        <li>Maintains grammatical structure</li>
                        <li>Some knowledge blending (e.g., mixing animal facts)</li>
                        <li>Temperature affects creativity vs. consistency</li>
                        <li>Occasional word fragments due to character-level nature</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    
    <script src="https://html2canvas.hertzen.com/dist/html2canvas.min.js"></script>
    <script>
        // Add smooth scrolling
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
        
        // Export diagram as image
        function exportDiagram() {
            const element = document.getElementById('diagram-container');
            const button = document.querySelector('.export-button');
            
            // Hide the export button temporarily
            button.style.display = 'none';
            
            // Configure html2canvas options
            const options = {
                scale: 2, // Higher resolution
                useCORS: true,
                backgroundColor: '#ffffff',
                width: element.scrollWidth,
                height: element.scrollHeight
            };
            
            html2canvas(element, options).then(canvas => {
                // Create download link
                const link = document.createElement('a');
                link.download = 'transformer_architecture_diagram.png';
                link.href = canvas.toDataURL('image/png', 1.0);
                
                // Trigger download
                document.body.appendChild(link);
                link.click();
                document.body.removeChild(link);
                
                // Show the button again
                button.style.display = 'block';
                
                // Show success message
                button.textContent = '‚úÖ Exported!';
                setTimeout(() => {
                    button.textContent = 'üì∏ Export as Image';
                }, 2000);
            }).catch(error => {
                console.error('Error exporting diagram:', error);
                button.style.display = 'block';
                button.textContent = '‚ùå Export Failed';
                setTimeout(() => {
                    button.textContent = 'üì∏ Export as Image';
                }, 2000);
            });
        }
        
        // Add hover effects for interactive elements
        document.querySelectorAll('.component').forEach(component => {
            component.addEventListener('mouseenter', function() {
                this.style.transform = 'translateY(-3px) scale(1.02)';
            });
            
            component.addEventListener('mouseleave', function() {
                this.style.transform = 'translateY(0) scale(1)';
            });
        });
        
        // Add click-to-highlight functionality
        document.querySelectorAll('.component').forEach(component => {
            component.addEventListener('click', function() {
                // Remove previous highlights
                document.querySelectorAll('.component').forEach(c => {
                    c.style.boxShadow = '';
                });
                
                // Highlight clicked component
                this.style.boxShadow = '0 0 20px #3498db';
                
                // Remove highlight after 3 seconds
                setTimeout(() => {
                    this.style.boxShadow = '';
                }, 3000);
            });
        });
    </script>
</body>
</html>